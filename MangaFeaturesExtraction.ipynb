{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "MangaFeaturesExtraction.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "TZqiWDnFrpxk"
   },
   "source": [
    "from IPython.display import clear_output"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t4TS12xGrDkU"
   },
   "source": [
    "# installing the needed libraries\n",
    "!pip install easyocr\n",
    "!pip install pyspellchecker\n",
    "!pip install sentence-transformers\n",
    "!pip install annoy\n",
    "clear_output()"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EtXl_KInsMIZ"
   },
   "source": [
    "# getting the manga data\n",
    "!unzip dataset.zip -d dataset\n",
    "clear_output()"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tQykDgkKr4HP"
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from re import findall\n",
    "from os import listdir\n",
    "from pathlib import Path\n",
    "from easyocr import Reader\n",
    "from annoy import AnnoyIndex\n",
    "from torch.cuda import is_available\n",
    "from spellchecker import SpellChecker\n",
    "from sentence_transformers import SentenceTransformer"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbNceYxAB2A9",
    "outputId": "4bd5d9db-86e0-4e98-9073-9fda3e4e4e3e"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HrJixmvArkSE"
   },
   "source": [
    "class MangaTextExtractor:\n",
    "    '''Simple class to extract text from manga page'''\n",
    "\n",
    "    def __init__(self, lang='ru', max_distance=1, confidence=0.1):\n",
    "        '''Get the content of folder with manga'''\n",
    "\n",
    "        # setting up how confident we should be in the text extraction\n",
    "        self.confidence = confidence\n",
    "\n",
    "        # deciding where to infer the model\n",
    "        self.GPU = True if is_available() else False\n",
    "\n",
    "        # initializing the reader\n",
    "        self.reader = Reader([lang], gpu=self.GPU)\n",
    "\n",
    "        # initializing the spellchecker\n",
    "        self.checker = SpellChecker(language=lang, distance=max_distance)\n",
    "\n",
    "    def get_text_from_page(self, page_file_name):\n",
    "        '''Return list of texts for each page in a folder'''\n",
    "\n",
    "        # getting the raw detection from easyocr\n",
    "        detection = self.reader.readtext(page_file_name)\n",
    "\n",
    "        # filtering out some predictions by confidence\n",
    "        detection = list(filter(lambda det: det[2] > self.confidence, detection))\n",
    "\n",
    "        # detecting the words presented in lowercase\n",
    "        words = findall(r'\\w+', \" \".join(list(map(lambda det: det[1].lower(), detection))))\n",
    "\n",
    "        # correcting the spellchecking of those words\n",
    "        misspelled = self.checker.unknown(words)\n",
    "\n",
    "        # replacing misspelled words with correct versions\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in misspelled:\n",
    "                words[i] = self.checker.correction(words[i])\n",
    "\n",
    "        # returning the corrected words\n",
    "        return words"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l0fpDPrAWyze"
   },
   "source": [
    "\"\"\"THE FOLLOWING CODE IS TAKEN FROM LAB 8 OF OUR COMPUTER VISION COURSE\"\"\"\n",
    "\n",
    "class YOLO:\n",
    "    def __init__(self, confidence=0.5, threshold=0.3):\n",
    "        \n",
    "        self.CONFIDENCE = confidence\n",
    "        self.THRESHOLD = threshold\n",
    "\n",
    "        # load the custom class labels our YOLO model was trained on\n",
    "        with open(f\"yolo/darknet.labels\", 'r') as f:\n",
    "            self.labels = f.read().split(\"\\n\")\n",
    "\n",
    "        # initialize a list of colors to represent each possible class label\n",
    "        np.random.seed(42)\n",
    "        self.COLORS = np.random.randint(0, 255, size=(len(self.labels), 3),\tdtype=\"uint8\")\n",
    "\n",
    "        # derive the paths to the YOLO weights and model configuration\n",
    "        weightsPath = f\"yolo/yolo_best.weights\"\n",
    "        configPath = f\"yolo/yolov4.cfg\"\n",
    "\n",
    "        # load YOLO object detector trained on our dataset (15 classes)\n",
    "        net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "        net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "        net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "        # determine only the *output* layer names that we need from YOLO\n",
    "        ln = net.getLayerNames()\n",
    "        ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "        self.net = net\n",
    "        self.ln = ln\n",
    "\n",
    "    def IoU(self, box1, box2):\n",
    "        # left -> rightmost\n",
    "        left_x = max(box1[0], box2[0])\n",
    "        # bottom - topmost\n",
    "        bottom_y = min(box1[3], box2[3])\n",
    "        # right - leftmost\n",
    "        right_x = min(box1[2], box2[2])\n",
    "        # top - bottommost\n",
    "        top_y = max(box1[1], box2[1])\n",
    "        \n",
    "        # compute intersection area\n",
    "        interArea = (top_y - bottom_y) * (right_x - left_x)\n",
    "\n",
    "        # compute the area of both the prediction and ground-truth\n",
    "        ground_area = (box1[2] - box1[0]) * (box1[1]-box1[3])\n",
    "        predicted_area = (box2[2] - box2[0]) * (box2[1]-box2[3])\n",
    "\n",
    "        areaOrNegative = interArea/(ground_area + predicted_area - interArea)\n",
    "\n",
    "        # compute the IoU\n",
    "        return max(0, areaOrNegative)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        (H, W) = image.shape[:2]\n",
    "        # construct a blob from the input image and then perform a forward\n",
    "        # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "        # associated probabilities\n",
    "        blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        layerOutputs = self.net.forward(self.ln)\n",
    "        \n",
    "        # initialize our lists of detected bounding boxes, confidences, and\n",
    "        # class IDs, respectively\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classIDs = []\n",
    "\n",
    "        for output in layerOutputs:\n",
    "            for detection in output:\n",
    "                # extract the class ID and confidence\n",
    "                scores = detection[5:]\n",
    "                classID = np.argmax(scores)\n",
    "                confidence = scores[classID]\n",
    "\n",
    "                # filter out weak predictions \n",
    "                if confidence > self.CONFIDENCE:\n",
    "                    # scale the bounding box coordinates back relative to the\n",
    "                    # size of the image, keeping in mind that YOLO actually\n",
    "                    # returns the center (x, y)-coordinates of the bounding\n",
    "                    # box followed by the boxes' width and height\n",
    "                    box = detection[0:4] * np.array([W, H, W, H])\n",
    "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                    # use the center (x, y)-coordinates to derive the top and\n",
    "                    # and left corner of the bounding box\n",
    "                    x = int(centerX - (width / 2))\n",
    "                    y = int(centerY - (height / 2))\n",
    "\n",
    "                    # update our list of bounding box coordinates, confidences,\n",
    "                    # and class IDs\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    classIDs.append(classID)\n",
    "                \n",
    "        self.boxes = np.array(boxes)\n",
    "        self.confidences = np.array(confidences)\n",
    "        self.classIDs = np.array(classIDs)\n",
    "    \n",
    "    def non_max_supression(self):\n",
    "        idxs = np.argsort(-self.confidences)\n",
    "        confidences = self.confidences[idxs]\n",
    "        boxes = self.boxes[idxs]\n",
    "        classIDs = self.classIDs[idxs]\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            x,y,w,h = boxes[i][0],  boxes[i][1], boxes[i][2], boxes[i][3]\n",
    "            box1 = [x, y, x+w, y+h]\n",
    "            for j in range(i+1, len(boxes)):\n",
    "                # If they are of the same class\n",
    "                # and have a IoU above self.THRESHOLD\n",
    "                # we regard them as describing the same object and\n",
    "                # set the confidence of the box with lower confidence to 0\n",
    "                x, y, width, height = boxes[j][0],  boxes[j][1], boxes[j][2], boxes[j][3]\n",
    "                box2 = [x, y, x+width, y+height]\n",
    "\n",
    "                if self.IoU(box1, box2) > self.THRESHOLD and classIDs[i] == classIDs[j]:\n",
    "                    if (confidences[i] >= confidences[j]):\n",
    "                        confidences[j] = 0\n",
    "                    else:\n",
    "                        confidences[i] = 0\n",
    "\n",
    "        idxs = np.where(confidences>0)\n",
    "        self.boxes = boxes[idxs]\n",
    "        self.confidences = confidences[idxs]\n",
    "        self.classIDs = classIDs[idxs]\n",
    "  \n",
    "    def detect(self, image):\n",
    "\n",
    "        self.forward(image)\n",
    "        self.non_max_supression()\n",
    "        \n",
    "        return self.classIDs"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0OASBgtczqE6"
   },
   "source": [
    "class MangaFeatureExtractor:\n",
    "    '''Class to extract the vectorized features from manga directory'''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Initializing the modules that to vectorizing'''\n",
    "\n",
    "        # modules that do text embeddings \n",
    "        self.text_extractor = MangaTextExtractor() # extracting\n",
    "        self.sbert = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2') # embedding\n",
    "\n",
    "        # yolo module that do extraction of visual features\n",
    "        self.yolo = YOLO(confidence=0.01, threshold=0.2)\n",
    "\n",
    "    def get_features(self, folder):\n",
    "        '''\n",
    "        Calculate features for all manga pages in specified directory\n",
    "        and return them in two separate dictionaries\n",
    "        '''\n",
    "\n",
    "        # recursively searching all the jpg and manga pages in the specified folders\n",
    "        pages_file_names_jpg = list(map(str, Path(folder).rglob(\"*.jpg\")))\n",
    "        pages_file_names_png = list(map(str, Path(folder).rglob(\"*.png\")))\n",
    "        pages_file_names = pages_file_names_jpg + pages_file_names_png\n",
    "\n",
    "        ###################\n",
    "        # TEXT EMBEDDINGS #\n",
    "        ###################\n",
    "\n",
    "        # getting text from the text extractor for all pages\n",
    "        manga_texts = list(map(lambda path: self.text_extractor.get_text_from_page(path),\n",
    "                               pages_file_names))\n",
    "        \n",
    "        # getting the list om embeddings of texts on manga pages\n",
    "        text_embs = list(map(lambda text: self.sbert.encode(\" \".join(text)) if text else None,\n",
    "                             manga_texts))\n",
    "\n",
    "        # wrapping features into dictionary for later usage\n",
    "        embeddings_dict = dict(zip(pages_file_names, text_embs))\n",
    "\n",
    "        #########################\n",
    "        # VISUAL OBJECTS COUNTS #\n",
    "        #########################\n",
    "\n",
    "        # structure to hold the counts of visual words\n",
    "        visual_words = []\n",
    "\n",
    "        # going through each page and getting counts of classes instances\n",
    "        for page_file_name in pages_file_names:\n",
    "\n",
    "            # opening the page\n",
    "            page = cv2.imread(page_file_name)\n",
    "\n",
    "            # getting predictions for a page\n",
    "            classes = self.yolo.detect(page)\n",
    "\n",
    "            # iniailizing a counter with amount of trainable classes\n",
    "            counter = [0 for _ in range(len(self.yolo.labels))]\n",
    "\n",
    "            # populating the counter with amoutns of class instances\n",
    "            for class_ in classes:\n",
    "                counter[class_] += 1\n",
    "\n",
    "            # appending counts to the overall structure if counts are not a zero vector\n",
    "            visual_words.append(None if all(count == 0 for count in counter) else counter)\n",
    "\n",
    "        # wrapping counter features in a dict\n",
    "        counts_dict = dict(zip(pages_file_names, visual_words))\n",
    "\n",
    "        return embeddings_dict, counts_dict"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r0jo83zssQnx"
   },
   "source": [
    "# initializing the manga feature extractor\n",
    "MFE = MangaFeatureExtractor()\n",
    "\n",
    "# preparing the data to be indexed\n",
    "query_texts, query_counts = MFE.get_features('dataset/Boruto')\n",
    "\n",
    "# geenrating the query data to be queried\n",
    "index_texts, index_counts = MFE.get_features('dataset')"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qf3ETRC7EM8d"
   },
   "source": [
    "class AnnoyIdx:\n",
    "    def __init__(self, text_idx, img_idx):\n",
    "        self.word_index, self.word_reverse = self.build_index(text_idx)\n",
    "        self.img_index, self.img_reverse = self.build_index(img_idx) \n",
    "        \n",
    "    def build_index(self, index, trees=10, dist='angular'):\n",
    "        \n",
    "        #https://stackoverflow.com/questions/24068306/is-there-a-way-to-remove-nan-from-a-dictionary-filled-with-data\n",
    "        # Cleaning index\n",
    "        index = clean_dict = {k: index[k] for k in index if not index[k] is None}\n",
    "        \n",
    "        # Extracting dimensionality of data\n",
    "        index_dim = len(next(iter(index.values())))\n",
    "        print(index_dim)\n",
    "        \n",
    "        # Initializing trees\n",
    "        result_tree = AnnoyIndex(index_dim, dist)\n",
    "        result_reverse = {}\n",
    "        total = 0\n",
    "        \n",
    "        # Inserting items\n",
    "        for idx, key in enumerate(index.keys()):\n",
    "            result_reverse[idx] = key\n",
    "            result_tree.add_item(idx, index[key])\n",
    "          \n",
    "        # Building trees\n",
    "        result_tree.build(trees)\n",
    "            \n",
    "        return result_tree, result_reverse\n",
    "    \n",
    "    def find_similar(self, text_dict, img_dict, top_n=5, nns=10):\n",
    "\n",
    "        # removing entries that do not contain features\n",
    "        text_dict = {file_name:feature for file_name, feature in text_dict.items() if feature is not None}\n",
    "        img_dict = {file_name:feature for file_name, feature in img_dict.items() if feature is not None}\n",
    "\n",
    "        # sctructure to hold prediction with their cosine similarities \n",
    "        predictions = []\n",
    "        for key in text_dict:\n",
    "            \n",
    "            # Extracting vectors\n",
    "            text = text_dict[key]\n",
    "            \n",
    "            # Getting top-k results\n",
    "            best_words, dist_words = self.word_index.get_nns_by_vector(text, nns, include_distances=True)\n",
    "            \n",
    "            # appending prediction with its metric distance to the prediction structure\n",
    "            for word, dist in zip(best_words, dist_words):\n",
    "                predictions.append((self.extract_name(self.word_reverse[word]), dist))\n",
    "        \n",
    "        # doing the same thing for visual features\n",
    "        for key in img_dict:\n",
    "\n",
    "            # Extracting vectors\n",
    "            image = img_dict[key]\n",
    "\n",
    "            # Getting top-k results\n",
    "            best_imgs, dist_imgs = self.img_index.get_nns_by_vector(image, nns, include_distances=True)\n",
    "\n",
    "            # appending prediction with its metric distance to the prediction structure\n",
    "            for img, dist in zip(best_imgs, dist_imgs):\n",
    "                predictions.append((self.extract_name(self.word_reverse[img]), dist))\n",
    "\n",
    "        # removing zeros and wrapping into dict\n",
    "        predictions_dict = dict()\n",
    "        for pred, dist in predictions:\n",
    "            if dist != 0:\n",
    "                if pred in predictions_dict and dist < predictions_dict[pred]:\n",
    "                    predictions_dict[pred] = dist\n",
    "                else:\n",
    "                    predictions_dict[pred] = dist\n",
    "\n",
    "        predictions = list(predictions_dict.items())\n",
    "        predictions.sort(key=lambda pred: pred[1])\n",
    "\n",
    "        return predictions[:top_n]\n",
    "            \n",
    "    def extract_name(self, name):\n",
    "        return name.split('/')[1]"
   ],
   "execution_count": 91,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "buHHko_1EQjj",
    "outputId": "95a25d74-2703-4efa-91d4-c94ed372ce94"
   },
   "source": [
    "# build an index\n",
    "index = AnnoyIdx(index_texts, index_counts)\n",
    "\n",
    "# generate top picks\n",
    "print('Top picks for Boruto')\n",
    "print(*index.find_similar(query_texts, query_counts), sep='\\n')"
   ],
   "execution_count": 92,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "768\n",
      "15\n",
      "Top picks for Boruto\n",
      "('Onepiece2', 0.5721490383148193)\n",
      "('Mobpsycho100', 0.5851652026176453)\n",
      "('Onepunchman', 0.6255282759666443)\n",
      "('JJBASteelBallRun', 0.6330125331878662)\n",
      "('ВеликийизБродячихпсов', 0.7457972168922424)\n"
     ]
    }
   ]
  }
 ]
}